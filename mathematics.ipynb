{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 机器学习的数学基础\n",
    "\n",
    "机器学习对于数学要求比较高，但是学习过程中也不能淹没在数学细节中。\t\n 机器学习的重要的数学基础之一是`线性代数`和`多变量微积分`（`Linear Algebra` and `Multivariable Calculus`）。\t\n 对于这门学科要掌握到什么程度?请弄懂以下三道试题即可。",
    "\n",
    "## 试题一.{Gradients and Hessians}",
    "\n",
    "### 题目",
    "\n",
    "Recall that a matrix $A \\in {R}^{n \\times n}$ is symmetric if $A^T=A$, that is, ${A}_{ij}={A}_{ji}$ for all $i, j$. Also recall the gradient $\\triangledown f(x)$ of a function $f:{R}^{n} \\rightarrow n$ which is the n-vector of partial derivatives $\\triangledown f(x) = \\begin{bmatrix} \\frac { \\partial  }{ \\partial { x }_{ 1 } }f(x) \\\\ \\vdots \\\\ \\frac { \\partial  }{ \\partial { x }_{ n }}f(x) \\end{bmatrix}$ where $x =  \\begin{bmatrix} {x}_{1} \\\\ \\vdots \\\\ {x}_{n} \\end{bmatrix}$.\t\n The hessian ${\\triangledown}^{2}f(x)$ of a function $f:{R}^{n} \\rightarrow R$ is the $ n \\times n$ symmetric matrix of twice partial derivatives, $\\begin{bmatrix} \\frac{{\\partial}^{2}}{\\partial {x}_{1}^{2}}f(x) & \\frac{{\\partial}^{2}}{\\partial {x}_{1} {x}_{2}}f(x) & \\cdots & \\frac{{\\partial}^{2}}{\\partial {x}_{1} \\partial {x}_{n}}f(x) \\\\ \\frac{{\\partial}^{2}}{\\partial {x}_{2}{x}_{1}}f(x) & \\frac{{\\partial}^{2}}{\\partial {x}_{2}^{2}}f(x) & \\cdots & \\frac{{\\partial}^{2}}{\\partial {x}_{2} \\partial {x}_{n}}f(x) \\\\ \\vdots & \\vdots & \\ddots &\\vdots \\\\ \\frac{{\\partial}^{2}}{\\partial {x}_{n}{x}_{1}}f(x) & \\frac{{\\partial}^{2}}{\\partial {x}_{n}{x}_{2}}f(x) & \\cdots & \\frac{{\\partial}^{2}}{\\partial {x}_{n}^{2}}f(x) \\end{bmatrix}$. \n",
    "\n",
    "### 第一问\n",
    "Let $f(x)=\\frac{1}{2}{x}^{T}Ax + {b}^{T}x$ where A is a symmetric matrix and $b \\in {R}^{n}$ is a vector. What is $\\triangledown f(x)$?\n",
    "### 解：\n",
    "A可以写成：$A = \\begin{bmatrix} {R}_{1} \\\\ {R}_{2} \\\\ \\cdots \\\\ {R}_{n} \\end{bmatrix}$ 其中 $ {R}_{i} = \\begin{bmatrix} {A}_{i1} & {A}_{i2} & \\cdots & {A}_{in} \\end{bmatrix} $\t\n 设:$ {f}_{1}(x) = {x}^{T}Ax = {x}^{T}(Ax)={x}^{T}(\\begin{bmatrix} {R}_{1} \\\\ {R}_{2} \\\\ \\cdots \\\\ {R}_{n} \\end{bmatrix}x) = {x}^{T}\\begin{bmatrix} {R}_{1}x \\\\ {R}_{2}x \\\\ \\cdots \\\\ {R}_{n}x \\end{bmatrix} = {x}^{T}\\begin{bmatrix} {R}_{1}x \\\\ {R}_{2}x \\\\ \\cdots \\ {R}_{n}x \\end{bmatrix} $ \t\n (其中：${R}_{i}$为行向量；$x$为列向量) \t\n $ = \\begin{bmatrix} {x}_{1} & {x}_{2} &  \\cdots  & {x}_{n}  \\end{bmatrix} \\begin{bmatrix} {R}_{1}x \\\\ {R}_{2}x \\\\ \\vdots \\\\ {R}_{n}x  \\end{bmatrix}  =\\sum_{i=1}^{n}{{x}_{i}{R}_{i}x} =\\sum_{i=1}^{n}{({R}_{i}x){x}_{i}} = \\sum_{i=1}^{n}{\\sum_{j=1}^{n}{{A}_{ij}{x}_{j}}{x}_{i}} = \\sum_{i=1}^{n}{\\sum_{j=1}^{n}{{A}_{ij}{x}_{i}}{x}_{j}} $\t\n 所以：$\\triangledown {f}_{1}(x)=\\begin{bmatrix} \\frac{\\partial}{\\partial {x}_{1}}({f}_{1}(x)) \\\\ \\frac{\\partial}{\\partial {x}_{2}}({f}_{1}(x)) \\\\ \\vdots \\\\ \\frac{\\partial}{\\partial {x}_{k}}({f}_{1}(x)) \\\\ \\vdots \\\\ \\frac{\\partial}{\\partial {x}_{n}}({f}_{1}(x)) \\\\ \\end{bmatrix} $ \t\n 通项：$\\frac{\\partial}{\\partial {x}_{k}}({f}_{1}(x))=\\frac{\\partial}{\\partial {x}_{k}}(\\sum_{i=1}^{n}{\\sum_{j=1}^{n}{{A}_{ij}{x}_{i}}{x}_{j}}) = part1 + part2 + part3 + part4$"
    ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
